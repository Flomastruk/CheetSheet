\documentclass[12pt]{article}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{mathtools}

\usepackage{wrapfig, tikz, tikz-cd}
\usetikzlibrary{arrows, arrows, calc, decorations.markings, automata,calc}

\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{colorlinks=false,linkbordercolor=red,linkcolor=green,pdfborderstyle={/S/U/W 2}}

\usepackage{url}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{conjecture}[theorem]{Conjecture}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\numberwithin{equation}{section}

\newcommand{\dd}{\partial}
\newcommand{\argmax}{\operatorname{argmax}}
\newcommand{\Span}{\operatorname{Span}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\KK}{\mathbb{K}}
\newcommand{\rr}{\text{r}}
\newcommand{\al}{\alpha}
\newcommand{\be}{\beta}
\newcommand{\eps}{\varepsilon}
\newcommand{\ph}{\varphi}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\BB}{\mathfrak{B}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\MM}{\mathfrak{M}}
\newcommand{\kk}{\textbf{k}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\VV}{\mathbb{V}}
\newcommand{\II}{\mathbb{I}}
\newcommand{\eq}{\stackrel{\mathclap{\normalfont\text{def}}}{=}}
\newcommand{\convas}{\stackrel{\mathclap{\normalfont\text{as}}}{\longrightarrow}}


\begin{document}

\section{General definitions}

\subsection{Basic}

\begin{itemize}
\item
Sample variance
\begin{equation}\label{sample_variance}
S_X^2 = \frac 1{n-1}\sum_{i = 1}^n(X_i - \bar X)^2
\end{equation}

\item
Sample correlation coefficient
\begin{equation}\label{sample_correlation}
r_{X,Y} = \frac{\sum_{i = 1}^{n}(X_i-\bar{X})(Y_i-\bar{Y})}{(n-1)\sqrt{(S_X^2S_Y^2)}}
\end{equation}

\item
QQ-plot for cumulative distribution function $F$ is the set of points $\left(q_F\left(\frac i {n+1}\right), x_{(i)}\right)$, where $q_F(\cdot)$ is the quantile function for the distribution.

\item Mean Squared Error (MSE)
\begin{equation}
	\text{MSE}(\theta; T(X), g(\theta)) = \mathbb{E}_\theta\left(T(X) - g(\theta)\right)^2
\end{equation}

\item
Bias-variance decomposition
\begin{equation}
\text{MSE}(\theta; T(X)) = \text{var}_\theta T + \left(\mathbb{E}_\theta T(X) - g(\theta)\right)^2
\end{equation}

\item\label{ecdf}
Empirical distribution function
\begin{equation}
	\hat{F_n}(x) = \sum_{i = 1}^n \II(X_i \leq x)
\end{equation}

\end{itemize}

\subsection {$k$-th order statistic $X_{(k)}$}
$X_{(k)}$ --- $k-th$ order statistic distribution for $n$ i.i.d. variables from continuous distribution $F$.
\begin{equation}\label{kth_order_pdf}
	f_{(k)}(x) = \frac{n!}{(k-1)!(n-k)!}F(x)^{k-1}(1-F(x))^{n-k}f(x)
\end{equation}
\begin{equation}\label{kth_order_cdf}
	F_{(k)}(x) = \sum_{j = k}^n {{n}\choose{j}}F(x)^j (1-F(x))^{n-j}
\end{equation}

\begin{equation}\label{kth_order_expectation}
	\mathbb{E}F(X_{(k)}) = \frac k{n+1}
\end{equation}

\subsection{Time Series}

Time series below are assumed to be \emph{weakly stationary} in the following sense:

\begin{itemize}
	\item Stochastic time series $X_t(\omega)$ are called \emph{weakly stationary}, if $\EE X_t$ and $\text{Cov}(X_t, X_s)$ are inependent of time shifts; in particular, it first and second moments exist.
	\item For a weakly stationary time series $X_t$, the following functios are defined:
	\begin{itemize}
		\item\emph{Autocovariance funciton}:
		\begin{equation}
			\gamma_X(h) = \EE(X_{t+h}, X_t)
		\end{equation}
		\item \emph{Autocorrelation function}
		\begin{equation}
			\rho_X(h) = \frac {\gamma_X(h)}{\gamma_X(0)}
		\end{equation}
		\item \emph{Partial autocorrelation function} $\varphi(h)$ is defined as the coefficient the regression of $X_{t+h}$ on $X_t$ when controlled for constant and $X_{t+1}, \ldots X_{t+h-1}$ (see Proposition \ref{pac_properties}). In particular, $\varphi(0) = 1$ and $\varphi(1) = \rho(1)$
	\end{itemize}

\end{itemize}



\section{Important distributions}
\begin{itemize}
	\item Student's $t$-distribution $t_\nu$, $\nu\in\RR_{>0}$
	\begin{itemize}
		\item pdf
		\begin{equation}\label{t_pdf}
			\frac{\Gamma(\frac{\nu+1}2)}{\sqrt{\pi\nu}\ \Gamma(\frac\nu2)}\left(1+\frac {x^2}\nu\right)^{-\frac{\nu+1}2}
		\end{equation}
		\item cdf
		\begin{equation}\label{t_cdf}
			\frac 12 +x\Gamma\left(\frac{\nu+1}2\right)\frac{_2F_1\left(\frac 12, \frac {\nu+1}2; \frac 32, -\frac {x^2}\nu\right)}{\sqrt{\pi\nu}\ \Gamma(\frac\nu2)}
		\end{equation}
		\item $t$-distribution with $n\in\NN$ degrees of freedom arises from the ratio of independent $N(0,1)$- and $\chi^2_n$-distributions
	\end{itemize}
	\item Poisson distribution Poisson$(\lambda),\ \lambda > 0$
	\begin{itemize}
		\item $\lambda$ is the average number of events per interval
		\item pdf
		\begin{equation}\label{poisson_pdf}
			p_\lambda(k) = e^{-\lambda}\frac {\lambda^k}{k!}
		\end{equation}
	\end{itemize}


	\item Geometric distribution $G(\theta),\ 0\leq\theta\leq 1$
	\begin{itemize}
		\item pdf
		\begin{equation}\label{geometric_pdf}
			f_\theta(k) = (1-\theta)^{1-k}\theta
		\end{equation}
		\item cdf
		\begin{equation}\label{geometric_cdf}
			F_\theta(k) = 1-(1-\theta)^k
		\end{equation}
	\end{itemize}

	\item Exponential distribution $F(x;\lambda)$
	\begin{itemize}
		\item pdf
		\begin{equation}\label{exponential_pdf}
			f_\lambda(x) = \lambda e^{-\lambda x}
		\end{equation}
		\item cdf
		\begin{equation}\label{exponential_cdf}
			F_\lambda(x) = 1 - e^{-\lambda x}
		\end{equation}
		\item $\EE_\lambda X = 1/\lambda$

	\end{itemize}

	\item Beta distribution $B(\alpha, \beta),\ \alpha, \beta > 0$
	\begin{itemize}
		\item pdf
		\begin{equation}\label{beta_pdf}
			f_{\alpha, \beta}(x) = \frac{x^{\alpha-1} (1-x)^{\beta-1}}{B(\alpha, \beta)},\ B(\alpha, \beta) \equiv \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}
		\end{equation}

	\end{itemize}

	\item Weibull distribution
	\begin{itemize}
		\item $\alpha$ and $\lambda$ are the ``shape'' and `inverse scale'' parameters.
		\item pdf
		\begin{equation}\label{weibull_pdf}
			f_{\lambda, \alpha}(x) = \lambda^{\alpha}\alpha x^{\alpha-1} e^{-(\lambda x)^{\alpha}}
		\end{equation}
		\item cdf
		\begin{equation}\label{weibull_cdf}
			F_{\lambda, \alpha}(x) = 1 - e^{-(\lambda x)^\alpha}
		\end{equation}
	\end{itemize}

	\item Gamma distribution $\Gamma(\alpha, \lambda),\ \alpha >0, \lambda > 0$
	\begin{itemize}
		\item $\alpha$ and $\lambda$ are known as ``shape'' and ``inverse scale'' parameters.
		\item pdf
			\begin{equation}\label{gamma_pdf}
				f_{\alpha, \lambda} (x)= \frac{x^{\alpha - 1} \lambda^{\alpha}e^{-\lambda x}}{\Gamma(\alpha)}
			\end{equation}
		\item cdf (where $\gamma(s, x) = \int_0^x t^{s-1}e^{-t}dt$ --- is the ``incomplete gamma function'')
			\begin{equation}
				F_{\alpha,\lambda}(x) = \frac{\gamma(\alpha, x\beta)}{\Gamma(\alpha)}
			\end{equation}
	\end{itemize}
\end{itemize}

\begin{definition}\label{exponential_family}
	A family of probability densities $p_\theta$ that depends on a parameter $\theta$ is called a $k$\emph{-dimensional exponential family} if there exist functions $c(\theta)$, $h(x)$, $Q_j(\theta)$, and $V_j(x)$ such that
	\begin{equation*}
		p_\theta(x) = c(\theta)h(x)e^{\sum_{j=1}^k Q_j(\theta)V_j(x)}
	\end{equation*}
\end{definition}

\section{Fundamental results}

\subsection{Convergence types and $o_p$- and $O_p$-notations}
\begin{definition}\label{convdef}
Let $X_n,\ n\in\NN$ be a sequence of random vectors.
	\begin{itemize}
		\item Sequence $X_n$ is said to \emph{converge in distribution} to a random vector $X$ if at any continuity point $x$ of c.d.f. $F_X$
		\begin{equation*}
			X_n \rightsquigarrow X \text{ if } \lim_{n\rightarrow \infty} F_{X_n}(x) = F_X(x)
		\end{equation*}
		\item Let $d(x,y)$ be a metric in a target space of $X_n$. Sequence $X_n$ \emph{converges in porobability} to a random vector $X$ if
		\begin{equation*}
 X_n \xrightarrow[]{p} X\text{ if } \PP(d(X_n, X) > \varepsilon)\xrightarrow[]{n\rightarrow\infty} 0, \forall \varepsilon >0
		\end{equation*}
		\item For the above notations, $X_n$ \emph{converges to $X$ almost surely} if
		\begin{equation*}
			 X_n\xrightarrow[]{as} X \text{ if } \PP\left(\lim_{n\rightarrow\infty}X_n = X\right) = 1
		\end{equation*}
	\end{itemize}
\end{definition}

\begin{theorem}
	Let $X_n, n\in\NN$ and $X$ be random vectors. Then
	\begin{enumerate}
		\item $X_n\xrightarrow[]{as} X$ implies $X_n \xrightarrow[]{p} X$
		\item $X_n\xrightarrow[]{p} X$ implies $X_n\rightsquigarrow X$
		\item $X_n\rightsquigarrow c$ for a constant $c$, if and only if $X_n\xrightarrow[]{p} X$
		\item If $X_n\rightsquigarrow X$ and $d(X_n, Y_n)\xrightarrow[]{p} 0$ then $Y_n\rightsquigarrow X$
		\item If $X_n\rightsquigarrow X$ and $Y_n\rightsquigarrow c$ for a constant $c$, then $(X_n, Y_n) \rightsquigarrow (X, c)$
		\item If $X_n\xrightarrow[]{p} X$ and $Y_n\xrightarrow[]{p} Y$ for a constant $c$, then $(X_n, Y_n) \xrightarrow[]{p} (X, Y)$
	\end{enumerate}
\end{theorem}

\begin{theorem}\label{cont_map_thm}\emph{(Strong law of large numbers)} Let $g:\RR^k\rightarrow\RR^m$ be continuous at every point of a set $c$ such that $\PP(X\in C) = 1$.
	\begin{enumerate}
		\item If $X_n\rightsquigarrow X$, then $g(X_n)\rightsquigarrow g(X)$
		\item If $X_n\xrightarrow[]{p} X$, then $g(X_n)\xrightarrow[]{p} g(X)$
		\item If $X_n\xrightarrow[]{as} X$, then $g(X_n)\xrightarrow[]{as} g(X)$
	\end{enumerate}
\end{theorem}

\begin{definition}
A family of random vectors $X_\al$ is called \emph{uniformally tight}~if
\begin{equation*}
	\lim_{M\rightarrow \infty} \sup_\alpha \Vert X_\alpha \Vert = 0
\end{equation*}
\end{definition}

\begin{theorem}\label{levys_thm}\emph{(Levy's continuity theorem)} Let $X_n$ and $X$ be random vectors with values in $\RR^k$, and let $\varphi_{X_n}(t)$ and $\varphi_{X}(t)$ be their characteristic functions respectively.
\begin{itemize}
\item If $\varphi_{X_n}(t)$ converges to $\varphi_{X}(t)$ pointwise then $X_n\rightsquigarrow X$
\item if $\varphi_{X_n}$ converges pointwise to a function continuous at $0$ then $X_n$ converges in distrubution to some random variable $X$ with that characteristic function
\end{itemize}
\end{theorem}

\begin{definition}
	Let $X_n$ and $R_n$ be sequences of random vectors. The following notations are used:
	\begin{itemize}
		\item $X_n = o_p(1)$ if $X_n\xrightarrow[]{p}0$
		\item $X_n = o_p(R_n)$ if $X_n = Y_nR_n$ and $Y_n = o_p(1)$ for some $Y_n$
		\item $X_n = O_p(1)$ if $X_n$ is uniformally tight
		\item $X_n = O_p(R_n)$ if $X_n = Y_nR_n$ and $Y_n = O_p(1)$ for some $Y_n$
	\end{itemize}
\end{definition}

\begin{proposition}
Let $R:\RR^k\rightarrow\RR^m$ be a function with $R(0) = 0$. Let $X_n\xrightarrow[]{p} 0$. Then for every $\alpha > 0$
\begin{enumerate}
	\item If $R(h) = o(\Vert h\Vert^\alpha)$ as $h\rightarrow 0$ then $R(X_n) = o_p(\Vert X_n \Vert^\alpha)$
	\item If $R(h) = O(\Vert h\Vert^\alpha)$ as $h\rightarrow 0$ then $R(X_n) = O_p(\Vert X_n \Vert^\alpha)$
\end{enumerate}
\end{proposition}

\begin{theorem}\label{delta_method}\emph{(Delta method)}
	Let $\varphi: \RR^k \rightarrow \RR^m$ be a map defined on a subset of $\RR^k$ and differentiable at $\theta$. Let $T_n$ be random vectors taking values in the domain of $\varphi$. If $r_n(T_n - \theta) \rightsquigarrow T$ for some numbers $r_n \xrightarrow[]{n\rightarrow\infty}\infty$, then \begin{equation*}
	r_n\left(\varphi(X_n) - \varphi(\theta)\right) \rightsquigarrow \frac{\partial}{\partial\theta}\varphi(\theta)(T)
	\end{equation*}
	where right hand side is the linear transformation of $T$ given by the differential of $\varphi$ at $\theta$.
\end{theorem}

\begin{example}
	Let $S_n$ be a sample variance of a sequence of i.i.d. random variables $X_n,\ n\in\NN$ with $\EE X_n = 0$ and finite fourth moment. Denote by $\kappa = \frac{\mu_4}{\mu_2^2} - 3$ kurtosis of $X_n$. Then
	\begin{equation*}
		\PP\left(\frac{nS^2}{\mu_2} > \chi^2_{n, \alpha}\right) = \PP\left(\sqrt n \left(\frac{S^2}{\mu_2}\right) > \frac{\chi^2_{n, \alpha} - n}{\sqrt n}\right) \rightarrow 1 - \Phi\left(\frac{z_\al\sqrt 2}{\sqrt{\kappa+2}}\right)
	\end{equation*}
\end{example}
\proof By central limit theorem \ref{clt}
\begin{equation*}
	\sqrt n \left(\begin{pmatrix}
		\bar{X_n}\\ \bar{X^2_n}
\end{pmatrix} - \begin{pmatrix}
	\al_1\\ \al_2
\end{pmatrix}\right) \rightsquigarrow N\left(\begin{pmatrix}
	0\\0
\end{pmatrix}, \begin{pmatrix}
	\al_2 - \al_1^2 & \al_3 - \al_1\al_2 \\
	\al_3 - \al_1\al_2 & \al_4 - \al_2^2
\end{pmatrix}\right)
\end{equation*}

\noindent As $S^2$ does not change if $X_n$ is replaced by its centered version, it can be assumed  that $\al_1 = 0$. Delta method for \ref{delta_method} for $\varphi(x, y) = y - x^2$ implies:

\begin{equation*}
	\sqrt{n}\left(S^2 - \mu_2 \right)\rightsquigarrow N(0, \mu_4 - \mu_2^2)
\end{equation*}

\subsection{Law of large numbers and central limit theorems}

\begin{theorem}\label{lln}\emph{(Strong law of large numbers)} Let $\bar{X_n}$ be the average of the first $n$ of a sequence of independent identically distributed random vectors $X_k,\ k\in\NN$. If $\EE \Vert X_k\Vert<\infty$ then $\bar{X_n}\convas\EE X_1$.
\end{theorem}

\begin{theorem}\label{clt}\emph{(Central Limit Theorem)} Let $\bar{X_n}$ be the average of the first $n$ of a sequence of i.i.d. random vectors $X_k,\ k\in\NN$. If $\EE \Vert X_k\Vert^2<\infty$ then
	\begin{equation}\label{clt_eq}
		\sqrt{n}(\bar{X_n} - \EE X_1) \rightsquigarrow N\left(0, \VV (X_1)\right)
	\end{equation}
\end{theorem}

\begin{theorem}\label{lf_clt}\emph{(Lindeberg-Feller theorem)}. For each $n$ let $X_{n,1},\ldots X_{n, k_n}$ be independent random vectors with $\EE \Vert X_{n,i}\Vert^2<\infty$ and such that:

	\begin{align*}
		\sum_{i=1}^{k_n}\EE\left(\Vert X_{n,i}\Vert^2\II\left(\Vert X_{n,i}\Vert>\varepsilon \right)\right) &\xrightarrow[]{n\rightarrow\infty} 0, \ \forall \varepsilon > 0\\
		\sum_{i=1}^{k_n} \VV(X_{n,i}) &\xrightarrow[]{n\rightarrow\infty} \Sigma
	\end{align*}

\noindent Then the sequence $\sum_{i = 1}^{k_n}\left(X_{n,i} - \EE X_{n,i} \right)$ converges in distributiton to $N(0, \Sigma)$.
\end{theorem}

\subsection{Basic statistics}

\begin{theorem}\label{normaliid}
	Let $X_1,\ldots X_n$ be an i.i.d. random variables from the $N(\mu, \sigma^2)$ distribution, then
	\begin{enumerate}
		\item $\bar X$ is $N(\mu, \sigma^2/n)$ distributed;
		\item $(n-1)S_X^2/\sigma^2$ is $\chi^2_{n-1}$-distributed (see \ref{sample_variance});
		\item $\bar X$ and $S^2_{X}$ are independent;
		\item $\sqrt{n}(\bar{X} - \mu)/\sqrt{S_X^2}$ has the $t_{n-1}$- distribution.
	\end{enumerate}
\end{theorem}
\proof $\left\|X\right\|^2 - n \bar{X}^2 = (n-1)S_X^2$

\begin{definition}
	Let $A\in\CC^{m\times n}$ and $A^{\sim}\in \CC^{n\times m}$. Consider a list of conditions:
	\begin{enumerate}
		\item $AA^{\sim}A = A$
		\item $A^{\sim}AA^{\sim} = A^{\sim}$
		\item $(AA^{\sim})^* = AA^{\sim}$
		\item $(A^{\sim}A)^* = A^{\sim}A$
	\end{enumerate}
	If $A^{\sim}$ satisfies the first condition, then it is a \emph{generalized inverese} of $A$. If it satisfies the first two conditions, then it is a \emph{reflexive generalized inverese} of $A$. If $A$ satisfies all four conditions the it is called the \emph{Moore-Penrose inverse}.
\end{definition}

\begin{proposition}
	The space of generalized inverse matrices to a given matrix has dimension $2 \operatorname{rk}A\operatorname{corank}A$. It is characterized by two properties:
	\begin{itemize}
		\item The restriction of $A^{\sim}$ on $\operatorname{Im}A$ is an arbitrary lift of the inverse of induced isomorphism on the quotient by $\operatorname{Ker}A$.
		\item The image of $A^{\sim}$ equals $A^{\sim} \operatorname{Im}A$.
	\end{itemize}
\end{proposition}

\begin{theorem}
	Let $X\sim N(\mu_X, \VV_X)$ be a $k$-dimensional random vector, where $\VV_X$ might be singular. Then for any linear condition of the form $BX = b$ and any linear transformation $AX$, conditional distributions of $X$ and $AX$ are given by the following formulae:
	\begin{align*}
		f(X|BX) \sim N(\mu_X + \VV_XB^T(B\VV_XB^T)^{\sim}(BX& - B\mu_X),\\ \VV_X - &\VV_XB^T(B\VV_XB^T)^{\sim}B\VV_X)\\
		f(AX|BX) \sim N(A\mu_X + A\VV_XB^T(B\VV_XB^T)^{\sim}(BX& - B\mu_X),\\ A\VV_XA^T - &A\VV_XB^T(B\VV_XB^T)^{\sim}B\VV_XA^T)
	\end{align*}
	where $A^{\sim}$ denotes reflexive generalized inverse of matrix $A$.
\end{theorem}

\proof Follows from the proposition below.

\begin{proposition}\label{gausscond2}
	Let $Y$ be a Gaussian random vecor such that
	\begin{equation*}
		Y = \begin{pmatrix}
			Y_1\\Y_2
	\end{pmatrix}\sim
	N\left(
	\begin{pmatrix}
		0\\0
	\end{pmatrix},
	\begin{pmatrix}
		V_{11} & V_{12}\\V_{21} & V_{22}
	\end{pmatrix}
	\right)
	\end{equation*}
	Then $Y_1$ conditional on $Y_2$ is Gaussian:
	\begin{equation*}
		f(Y_1|Y_2) \sim N(V_{12}V_{22}^{\sim}Y_2, V_{11} - V_{12}V_{22}^{\sim}V_{21})
	\end{equation*}
\end{proposition}
\begin{example}
Let
	\begin{equation*}
		\begin{pmatrix}
			x\\y
	\end{pmatrix}\sim
	N\left(
	\begin{pmatrix}
		0\\0
	\end{pmatrix},
	\begin{pmatrix}
		1 & \rho\\ \rho & 1
	\end{pmatrix}
	\right)
	\end{equation*}
	Then
	\begin{equation*}
			f(y|x) \sim N(\rho x, 1 - \rho^2)
	\end{equation*}
\end{example}
\noindent\emph{Proof of Proposition \ref{gausscond2}.} We only consider the case with $\VV(Y)$ nonsingular. It is generalized in a straightforward way, but does not admit straightforward matrix notation for Gaussian distributions.

By definition,
\begin{equation}\label{gausscond2eq1}
	f(Y_1|Y_2=y_2) = \frac{f(Y_1,y_2)}{\int_{Y_2 = y_2}f(Y_1, y_2)dY_1}
\end{equation}
It is enough to show that
\begin{align}\label{gausscond2eq2}
	Y^T\VV(Y)^{-1}Y = (Y_1 - V_{12}V_{22}^{-1})^T(V_{11}  - V_{12}V_{22}^{-1}V_{21})^{-1}(Y_1 - V_{12}V_{22}^{-1}) + C(Y_2)
\end{align}
where $C(Y_2)$ is a term that does not depend on $Y_1$. Note that in (\ref{gausscond2eq1}) the part of the exponential in Gaussian density depending on $C(Y_2)$ cancels out. After the cancellation, we are left with the required density up to a multiplicative constant. The latter has to have correct magnitude so that the whole expression corresponds to a density function; alternatively it can be computed directly.

The equation (\ref{gausscond2eq2}) is a direct consequence of the inversion formula for $2\times 2$ block matrix.



\subsection{Reminder on different convergence types}
\begin{definition}
	Let $X_n$ be a sequence of random variables defined on the probability space $\left(\Omega, \PP\right)$:
	\begin{itemize}
		\item $X_n$ is said to converge to $X$ \emph{almost surely} if $\PP(\lim_{n\rightarrow \infty}X_n = X) = 1$
		\item convergence in probability
		\item weak convergence
		\item $L_p$-convergence
	\end{itemize}
\end{definition}

\begin{theorem}
	If a sequence of random variables converges almost surely then it converges in probability.
\end{theorem}

\begin{proposition}
	Assume $\sum_{n\in\ZZ}\EE|X_n| < \infty$, then $\sum_{n\in\ZZ}X_n$ is defined as an almost sure limit and:
	\begin{equation*}
		\EE\sum_{n\in\ZZ}X_n = \sum_{n\in\ZZ}\EE X_n
	\end{equation*}
\end{proposition}
\proof Established using Monotone and Dominated Convergence theorems applied to partial sums of random variables.


\begin{proposition}
	Let $f(x)$ be a function of period $T$ that is absolutely integrable on the interval $[-T/2, T/2]$. Define its Fourier coefficients as follows:
\begin{align*}
	a_k &= \frac 2 T \int_{-T/2}^{T/2} f(x)\cos \frac {2\pi kx}T dx, \ k = 0,1, \ldots \\
	b_k &= \frac 2 T \int_{-T/2}^{T/2} f(x)\sin \frac {2\pi kx}T dx, \ k = 1,2, \ldots
\end{align*}
Then the partial sum $S_n(x) \eq a_0/2 + \sum_{k=1}^n \left( a_k \cos {\frac {2\pi kx}T}+b_k \sin {\frac {2\pi kx}T}\right)$ of the Fourier series admits an integral presentation:
\begin{equation*}
	S_n(x) = \frac 2 T \int_{-T/2}^{T/2} f(x+u) \frac {\sin(n+\frac 1 2)\frac{2\pi u}{T}}{\sin\frac {\pi u}{T}}dx
\end{equation*}
\end{proposition}
\emph{Remark:} Note that the coefficient $a_0$ defined by the formulas above is twice the coefficient to of the constant function in Fourier series (as the norm of a sine wave is twice less).

\begin{theorem}
	Let $f(x)$ be a function of period $T$ that is absolutely integrable on the interval $[-T/2, T/2]$.
	\begin{enumerate}
		\item At a point of continuity where $f(x)$ has a right and a left derivative, the Fourier series coverge absolutely to the value $f(x)$
		\item If $f(x)$ is continuous and its derivative $f'(x)$ is square integrable, then the Fourier series converge to $f(x)$ absolutely and uniformly
		\item If $f(x)$ is continuous, then the Fourier series are uniformally summable to $f(x)$ by the method of Ces\`{a}ro
	\end{enumerate}
\end{theorem}

\begin{theorem}
	If series $\sum_{k=1}^{\infty}\left(|a_k| + |b_k|\right)$ are absolutely summable then the associated trigonometric series
\begin{equation*}
\frac {a_0} 2 + \sum_{k=1}^\infty \left( a_k \cos {\frac {2\pi kx}T}+b_k \sin {\frac {2\pi kx}T}\right)
\end{equation*}
	converges absolutely and uniformly to a continuous periodic function of period $T$ of which it is a Fourier series
\end{theorem}

\begin{proposition}
	Let $(a_n)_{n\in\ZZ}$ be an element of $l^1(\ZZ)$, and let $(Z_t)_{t\in\ZZ}$ be a sequence of random variables satisfying $\EE |Z_t| < C_1\ \forall t$ for some constant $C_1$. Then the convolution
	\begin{equation*}
		X_t = \sum_{n\in\ZZ}a_nZ_{t-n}
	\end{equation*}
	is defined almost surely $\forall t\in \ZZ$.

	Moreover, if there exists a constant such that $\EE Z_t^2 < C_2\ \forall t$, then $X_t$ is also a limit in $L^2$-norm and $\EE X_t^2\leq |a_n|_1^2C_2$.
\end{proposition}

\begin{theorem}
	Let $(X_t)_{t\in \ZZ}\in L^2\left(\Omega, \PP, \CC^{d_X}\right)$ and $(Y_t)_{t\in \ZZ}\in L^2\left(\Omega, \PP, \CC^{d_Y}\right)$. Let $(a_m)_{m\in\ZZ} \in l^1(\ZZ, \CC^{d'_X\times d_X})$ and $(b_n)_{n\in\ZZ} \in l^1(\ZZ, \CC^{d'_Y\times d_Y})$. Then
	\begin{enumerate}
		\item Convolutions $\left(a*X\right)_t$ and $\left(b*Y\right)_t$ are well-defined elements of $L^2\left(\Omega, \PP, \CC^{d'_X}\right)$ and $L^2\left(\Omega, \PP, \CC^{d'_Y}\right)$.
		\item \begin{equation*}
			\gamma_{a*X,b*Y}(h) = \sum_{m,n\in\ZZ}a_m\gamma_{X,Y}(h+n-m)\bar{b}_n^T %= (a*\gamma_{X,Y}*\bar b^T)_h % not true
		\end{equation*}
	\end{enumerate}
\end{theorem}

\begin{corollary}
	 Let $(a_m),\ (b_n)\in l^1(\ZZ)$ and $(e_t)_{t\in\ZZ}$ be a sequence of uncorrelated $(0, \sigma^2)$ random variables. Denote Let $X_t = (a*e)_t,\ Y_t = (b*X)_t$. Then
	\begin{enumerate}
		\item
		\begin{equation*}\label{gamma_conv1}
			\gamma_X(h) = \sum_{n\in\ZZ}a_ma_{m-h}\sigma^2
		\end{equation*}
		\item
		\begin{equation}\label{gamma_conv2}
			\gamma_Y(h) = \sum_{n\in\ZZ}c_nc_{n-h}\sigma^2,
		\end{equation}
	where $c_n = (a*b)_n$.
	\end{enumerate}
\end{corollary}

\begin{theorem}\label{convl1l2}
	Let $(a_m)\in l^1(\ZZ),\ (b_n)\in l^2(\ZZ)$ and $(e_t)_{t\in\ZZ}$ be a sequence of uncorrelated $(0, \sigma^2)$ random variables. Denote $X_t = (a*e)_t$, then
	\begin{enumerate}
		\item For each $t\in\ZZ$ random variable $Y_t$ defined as $L_2$-limit of the sequence $\sum_{n = - N}^Nb_nX_{t-n}$ is well defined.
		\item  Formula (\ref{gamma_conv2}) holds for $Y_t$.
	\end{enumerate}
\end{theorem}

\begin{corollary}
	For $(b_n)\in l^2(\ZZ)$ the sequence $(b*e)_t$ is defined in $L^2(\Omega, \PP)$
\end{corollary}

\begin{theorem}
	Conclusions of Theorem \ref{convl1l2} hold for $(a_m)\in l^2(\ZZ),\ (b_n)\in~l^1(\ZZ)$
\end{theorem}



\subsection{Time Series}

\begin{proposition}\label{diff_sol}
	Given the difference equation of order $n$:
	\begin{equation}\label{diff_sol_rec}
		y_t + a_1y_{t-1} + a_2y_{t-2} + \ldots + a_ny_{t-n} = r_t, \ t = n, n+1, \ldots
	\end{equation}
	The solution $\left(y_n\right)_{n = 0}^{\infty}$ can be expressed in the form:
	\begin{equation*}
		y_t =  \sum_{i = 0}^t w_ir_{t-i}, \ t = 0, 1, \ldots
	\end{equation*}
	with $w_i\equiv0$ for $i<0$, and satisfying the homogeneous differrence equaition of $(y_t)$:
	\begin{equation}\label{diff_sol_rel}
		\sum_{i = 0}^n a_iw_{t-i} = 0,\ i = 1, 2,\ldots
	\end{equation}

\end{proposition}
\proof
Using recursive formula (\ref{diff_sol_rec}) for $y_t$, any element of the sequence can be written in the form:
\begin{equation*}
	y_t = \sum_{i = 0}^t w_i^{(t)} r_{t-i}
\end{equation*}
for some $w_i^{(t)}$ with $t\in\ZZ_{\geq 0},\ 0\leq i\leq t$.
Using inductive argument, we show that
\begin{enumerate}
	\item $w_i^{(t)}\equiv w_i$ for some constant depending on $i$ only
	\item The sequence $w_i$ satisfies equation (\ref{diff_sol_rel})
\end{enumerate}

Indeed, from the equation \ref{diff_sol_rec} it is immediate to see that $w_0 = 1$ and $w_1 = -a_1$. Assuming that the pair of statements above is shown for $s < t$, write:

\begin{equation}\label{diff_sol_pr}
	y_t = r_t - \sum_{k = 0}^{n}a_k^{(t)}y_{t-k}=r_t - \sum_{k = 1}^{t}a_k\sum_{j = 0}^{t - k} w_jr_{t - k - j}
\end{equation}
By definition, we set $w_t$ to be the coefficient of $r_0$, that is,  $w_t = - \sum_{i = 1}^n a_iw_{t-i}$. Moreover, by rearranging terms in \ref{diff_sol_pr}, we get:
\begin{equation*}
	y_t = -\sum_{j = 0}^t r_j \sum_{k = 1}^{t-j}{a_k w_{t-j-k}},
\end{equation*}
it follows that the coefficient $w_{t-j}^{(t)}$ of $r_j, \ j = 1, \ldots t,$ is equal to $w_{t-j}$ by induction assumpiton and it concludes the proof.


\begin{theorem}\label{infARinfMA}
	\begin{enumerate}
	\item
	Let $X_t$ be an $AR(p)$ process given by
	\begin{equation*}
		X_t + \sum_{j = 0}^p a_j X_{t-j} = e_t,
	\end{equation*}
	where $(e_t)_{t\in\ZZ}$ is a sequence of uncorrelated $(0, \sigma^2)$ random variables. Suppose that all roots of the polynomial
	\begin{equation*}
		m^p+\sum_{j = 0}^p a_j m^{p-j}
	\end{equation*}
	have magnitude less than $1$.
	Then $X_t$ admits an infinite $MA$-presentation $X_t = \sum_{j=0}^\infty w_j e_{t-j}$, where
	\begin{align*}
		w_0 &= 1\\
		w_j &+ \sum_{i = 1}^j a_iw_{j-i} = 0,\ j = 1,\ldots p-1\\
		w_j &+ \sum_{i = 1}^p a_iw_{j-i} = 0,\ j = p,p+1,\ldots
	\end{align*}
	\item
	Let $X_t$ be an $MA(q)$ process given by
	\begin{equation*}
		X_t = e_t + \sum_{j = 0}^q b_j e_{t-j}
	\end{equation*}
	where $(e_t)_{t\in\ZZ}$ is a sequence of uncorrelated $(0, \sigma^2)$ random variables. Suppose that all roots of the polynomial
	\begin{equation*}
		m^p+\sum_{j = 0}^q b_j m^{q-j}
	\end{equation*}
	have magnitude less than $1$.
	Then $X_t$ admits an infinite $AR$-presentation $\sum_{j=0}^\infty c_jX_{t-j} =  e_t$, where
	\begin{align*}
		c_0 &= 1\\
		c_j &+ \sum_{i = 1}^j b_ic_{j-i} = 0,\ j = 1,\ldots q-1\\
		c_j &+ \sum_{i = 1}^p b_ic_{j-i} = 0,\ j = q,q+1,\ldots
	\end{align*}
\end{enumerate}
\end{theorem}

\begin{proposition}
	Let $X_t$ be an $AR(p)$ process satisfying conditions of Theorem \ref{infARinfMA}. Then the partial autocorrelation function $\varphi(h) = 0$ for $h > p$
\end{proposition}
\proof By definition, $\varphi(h)$ is the correlation between  the $X_{t-h}$ and the residual obtained from the regression of $X_t$ on $X_{t-1},\ldots X_{t-h+1}$. The latter is $e_t$ and the result follows.

\begin{proposition}
	Let $X_t$ be an $AR(p)$ process satisfying conditions of Theorem \ref{infARinfMA}. Then autocovariance function $\gamma(h)$ satisfies:
	\begin{align*}
		\gamma(0) + a_1\gamma(1) +\ldots +a_p\gamma(p) &= \sigma^2\\
		\gamma(h) + a_1\gamma(h-1) +\ldots +a_p\gamma(h-p) &= 0,\ h>0
	\end{align*}
\end{proposition}
\noindent\emph{Remark:} this result allows one to express $\gamma(0), \ldots \gamma(p)$ through the coefficients $a_1,\ldots a_p$ and vice versa.

\begin{proposition}\label{pac_properties} Let $X_t$ be weakly stationary time series:
	\begin{enumerate}
		\item The partial autocorrelation coefficient $\varphi(h)$ equals $\theta_{hh}$ in the linear regression:
		\begin{equation*}
			X_{t+h} = \theta_{0h} + \theta_{1h}X_{t+h-1} + \ldots + \theta_{hh}X_t + a_{ht}
		\end{equation*}
		\item Let $\rho_{t+h,t\cdot(t+1, \ldots t+h-1)}$ denote the partial correlation of $X_{t+h}$ and $X_t$ when controlled for $X_{t+1}, \ldots X_{t+h-1}$. The squared norm of the residual term in the regression above equals:
		\begin{equation*}
			\EE(a_{ht}^2) = \gamma(0)\prod_{i = 1}^{h}(1-\rho_{t+h, t+i-1\cdot(t+i, \ldots t+h-1)}^2)
		\end{equation*}
	\end{enumerate}
\end{proposition}
\proof These statements follow from basic Eucledian geometry. Denote by $P$ the projection onto the subspace spanned by $X_{t+i}, \ldots X_{t+h-1}$. Now consider sequentially orthogonal decompositions $X_{t+h} = P(X_{t+h}) + (1 - P)(X_{t+h})$ and look at the component of the second summand along $(1 - P)(X_{t+i})$.


\begin{proposition}\label{tsOLS}
Let $(Y_t)_{t\in\ZZ}$ be a sequence of elements in $L^2\left(\Omega, \PP, \CC\right)$, denote $\textbf{Y}_n \equiv (Y_1, \ldots, Y_n)$. Let $\hat Y_{n+s}(Y_1,\ldots Y_n) \equiv \textbf{Y}_n b_{n,s}, \ b_{n,s}\in \CC^n$ be a linear predictor minimizing mean squared error
\begin{equation*}
	\tau_{n,s}^2\eq\text{MSE}\left(Y_{n+s},\hat Y_{n+s}(Y_1,\ldots Y_n)\right)\equiv  \EE\left\{\|Y_{n+s} - \hat Y_{n+s}\|^2\right\}
\end{equation*}
Then $b_{n,s} = (\VV_{n,n})^{+}V_{n, s}$ is a solution, where
\begin{align*}
	V_{n,s} &\eq \EE\left(\textbf{Y}_n^T Y_{n+s}\right)\\ \VV_{n,n} &\eq \EE\left(\textbf{Y}_n^T \textbf{Y}_n\right)
\end{align*}
Furthermore, MSE is given by:
\begin{equation*}
	\tau_{n,s}^2 = \VV(Y_{n+s}) - b_{n,s}^TV_{n,s} = \VV(Y_{n+s}) - V_{n,s}^T \VV_{n,n}^+V_{n,s}
\end{equation*}
\end{proposition}
\proof Follows from standard linear regression theory.

\begin{definition}
	A time-series is \emph{nonsingular (regular, nondeterministic)} if the sequence of mean squared errors of one-preriod prediction $\tau_{n,1}^2$ is bounded away from zero. A time series is \emph{singular (deterministic)} if
	\begin{equation*}
		\lim_{n\rightarrow \infty} \tau_{n,1} = 0
	\end{equation*}
\end{definition}

\begin{theorem}
Let $Y_t, b_{n,s}$  and $\tau_{n,s}$ be as defined in Proposition \ref{tsOLS} and assume that $Y_t$ is weakly stationary, nondeterministic. Denote the components of $b_{n,s}$ by $b_{n,s,i},\ i=1,\ldots n$ so that
\begin{equation*}
	\hat Y_{n+s}(Y_1,\ldots Y_n) = \sum_{i = 1}^n b_{n,s,i}Y_i
\end{equation*}
Then the following recursive relations take place:
\begin{enumerate}
	\item $$b_{n,s,1} = \tau_{n-1,1}^{-2}\left(\gamma(n+s-1) - \sum_{i=1}^nb_{n-1,s,i}\gamma(n+s-1-i)\right)$$
	\item $\tau_{n,s}^2 = \tau_{n-1,s}^2 - b_{n,s,1}\tau_{n-1,1}^2$
	\item $
	\begin{pmatrix} b_{n,s,2} \\ b_{n,s,3}\\ \vdots\\b_{n,s,n}\end{pmatrix} =
	\begin{pmatrix} b_{n-1,s,2} \\ b_{n-1,s,3}\\ \vdots\\b_{n-1,s,n} \end{pmatrix} - b_{n,s,1}
	\begin{pmatrix}
		b_{n-1,1,n-1} \\ b_{n-1,1,n-2}\\ \vdots\\b_{n-1,1,1}
	\end{pmatrix}$
\end{enumerate}
\end{theorem}
\noindent\emph{Remark.} Note that one-step prediction terms, $\tau_{n-1,1}$ and $b_{n-1,1}$, appear in the recursion, and the components of the last vector are reversed.

\begin{theorem}\emph{(Gram-Schmidt)} Let
	Let $(Y_t)_{t\in\ZZ}$ from $L^2\left(\Omega, \PP, \CC\right)$ be a zero-mean, stationary, nondeterministic time series.

Then one can write $Y_t = \sum_{i=1}^t c_{t,i}Z_i$ where
\begin{align*}
	\EE& Z_{t,i} = 0\\
	\EE& \left(Z_{t,i}Z_{t,j}\right) =\delta_{ij}\kappa_i^2\\
	c_{t,1} &= \kappa_1^{-2}\gamma_Y(t-1)\\
	c_{t,i} &= \kappa_{i}^{-2}\left(\gamma_Y(t-i) - \sum_{j<i}c_{t,j}c_{i,j}\kappa_i^2\right)\\
	\kappa_t^2 &= \gamma_Y(0) - \sum_{i = 1}^{t-1}c_{t,i}^2\kappa_i^2
\end{align*}
\end{theorem}
\proof This is a result of direct application of Gram-Schmidt orthogonalization algorythm to the sequence $Y_1, \ldots, Y_t$. Note that the process can be generalized to vector-valued processes.


\begin{theorem}
	The real valued function $\rho(h)$ is the correlation function of a real valued stationary time series $X_t(\omega)$ with index set $t\in\ZZ$ if and only if it is representable in the form
\begin{equation*}
	\rho(h) = \int_{-\pi}^\pi e^{ihx}dG(x),
\end{equation*}
	where $G(x)$ is a symmetric distribution function
\end{theorem}

\begin{theorem}
	Let the correlation funciton $\rho(h)$ of a stationary time series be abcolutely summable. Then there exists a continuous function $f(\omega)$ such that.
	\begin{enumerate}
		\item $\rho(h) = \int_{-\pi}^{\pi}f(\omega)\cos \omega h d\omega$
		\item $\int_{-\pi}^{\pi}f(\omega)d\omega = 1$
		\item $f(\omega)\geq 0$
		\item $f(\omega)$ is an even function
	\end{enumerate}
\end{theorem}


\subsection{Estimator convergence via information matrix}

\begin{definition}\label{information_def}
Let $X$ be a random variable defined on probability space $(\Omega,\PP_\theta),\ \theta\in\Theta$. Suppose that the likelihood function $\theta\mapsto\ell_\theta \eq \log p_\theta$ is differentiable for all $x\in\Omega$. The gradient
\begin{equation*}
	\dot{\ell_\theta}(x) = \frac{\partial}{\partial\theta}\ell_\theta(x)
\end{equation*}
is called the \emph{score function}. The \emph{Fisher information} is defined as the matrix
\begin{equation*}
	i_\theta = \VV_\theta\left(\dot{\ell_\theta}(X)\right)
\end{equation*}
\end{definition}

\begin{theorem}
	Suppose that $\Theta$ is compact and convex and that $\theta$ is identifiable, and let $\hat{\theta}_n$ be the maximum likelihood estimator based on a sample of size $n$ from the distribution with (marginal) probability density $p_\theta$. Suppose, furthermore, that the map $\vartheta\mapsto \log p_\vartheta(x)$ is continuously differentiable for all $x$, with derivative $\dot{\ell_\vartheta}(x)$, such that $\|\dot{\ell_\vartheta}(x)\|\leq L(x)$ for every $\vartheta\in\Theta$, where $L(x)$ is a function with $\EE_\theta L^2(X)<\infty$.
	If $\theta$ is an interior point of $\Theta$ and the function $\theta\mapsto i_\theta$ is continuous and positive, then under $\theta$, $\sqrt{n}(\hat{\theta}_n - \theta)$
	converges in distribution to a normal distribution with expectation $0$ and variance $i^{-1}_\theta$. Therefore, under $\theta$, as $n\rightarrow\infty$, we have
	\begin{equation*}
		\sqrt{n}(\hat{\theta}_n - \theta) \rightarrow N(0, i_\theta^{-1})
	\end{equation*}
\end{theorem}

\begin{theorem}\label{cramerraobound}\emph{(Cramer-Rao)}
	Suppose $\theta\mapsto p_\theta(x)$ is differentiable for every $x$. Then under certain regularity conditions any unbiased estimator $T$ for $g(\theta)$ satisfies:
	\begin{equation*}
		\VV_\theta(T) \geq g'(\theta)I_\theta^{-1}g'(\theta)^T,
	\end{equation*}
	where $I_\theta$ denotes the \emph{full} information matrix.
\end{theorem}


\subsection{Sufficient Statistics and UMVU estimators}

\begin{definition}
	For a statistical model $(\Omega, \PP_\theta), \ \theta \in \Theta$, a statistic $V(x)$ is called \emph{sufficient} (for $X$) if conditional distribution $f(x|V = v)$ is independent of $V$.
\end{definition}

\begin{theorem}\label{factorizarion}
	A statistic $V(x)$ is sufficient if there exist functions $h(x)$ and $g(v, \theta)$ such that
	$$
	p_\theta(x) = h(x)g(V(x), \theta)
	$$
\end{theorem}

\begin{theorem}\emph{(Rao-Blackwell)}
	Let $V = V(X)$ be a sufficient statistic, and let $T = T(X)$ be an arbitrary real-valued estimator for $g(\theta)$. Then there exists an estimator $T^* = T^*(V)$ for $g(\theta)$ that depends only on $V$, such that $\EE_\theta T^* = \EE_\theta T$ and $\VV_\theta T^* \leq \VV_\theta T$ for all $\theta$. In particular, we have $\text{MSE}(\theta; T^*) \leq \text{MSE}(\theta; T)$. This inequality is strict unless $\PP_\theta(T^* = T) = 1$.
\end{theorem}

\begin{definition}
For a statistical model $(\Omega, \PP_\theta), \ \theta \in \Theta$, a statistic $V(x)$	is called complete if $\EE_\theta(f(V)) = 0,\ \forall \theta\in \Theta$ implies $f(V)=0$ a.s.
\end{definition}

\begin{theorem}
	Let $V(x)$ be sufficient and complete, and $T(V)$ be an unbiased estimator for $g(\theta)$. Then $T(V)$ is UMVU estimator (i.e. has smallest variance among all unbiased estimators $\forall\theta\in\Theta$).
\end{theorem}

\begin{theorem}
	Suppose that for a $k$-dimensional exponential family (\ref{exponential_family}) the set below contains an interior point:
\begin{equation*}
	(Q_1(\theta),\ldots Q_k(\theta)),\ \theta\in\Theta
\end{equation*}
Then the random vector $\left(V_1(x),\ldots V_n(x)\right)$ is sufficient and complete.
\end{theorem}


\section{Estimators}

\subsection{Maximum of $n$ uniformally distributed statistics}

Set up: $X_1, X_2,\ldots X_n$ i.i.d. drown from $U[0, \theta]$, where $\theta$ is the parameter of interest.

\begin{itemize}
	\item $\hat\theta = 2\bar{X_n}$
	\begin{itemize}
		\item method of moments estimator
		\item \emph{unbiased}
		\item $\text{MSE}(\theta, \hat\theta) = \frac {\theta^2}{3n}$, see (\ref{kth_order_pdf})
	\end{itemize}
	\item $X_{(n)}$ --- $n$-th order statistic, i.e. maximum.
	\begin{itemize}
		\item $\mathbb{E}_\theta X_{(n)} = \frac{n}{n+1}\theta$, see (\ref{kth_order_pdf})
		\item $\text{MSE}(\theta, X_{(n)}) = \frac {2\theta^2}{(n+2)(n+1)}$
	\end{itemize}
	\item $\frac{n+2}{n+1}X_{(n)}$
	\begin{itemize}
		\item best estimator of the form $cX_{(n)}$
		\item $\text{MSE}(\theta, \frac{n+2}{n+1}X_{(n)}) = \frac {\theta^2}{(n+1)^2}$
	\end{itemize}
\end{itemize}

\subsection{Univariate normal distribution}

\begin{itemize}
	\item $\displaystyle{(\hat\mu, \hat{\sigma}^2) = \left(\bar{X_n}, \frac 1 n \sum_{i=1}^n (X_i - \bar{X_n}) \right) = \left(\bar{X_n}, \frac {n-1} n S_X^2 \right)}$
	\begin{itemize}
		\item maximum likelihood estimator
		\item method of moments estimator
		\item $\hat\mu$ is \emph{unbiased}
		\item $\mathbb{E}_{(\mu,\sigma^2)}\hat{\sigma}^2 = \frac {n-1} n \sigma^2$

	\end{itemize}

\end{itemize}

\subsection{Empirical distribution function}
Let $X_1, \ldots X_n$ be an i.i.d. sample drawn from the distribution $F$.
\begin{itemize}
	\item The empirical distribution function (ecdf) $\hat{F}(x) = \sum_{i = 1}^n \II(X_i \leq x)$ (see \ref{ecdf})
	\begin{itemize}
		\item \emph{unbiased}
		\item $\textbf{cov}_F\left(\hat F (u), \hat F (v)\right) = n^{-1}(F(\min(u, v)) - F(u)F(v))$ -- positively correlated
	\end{itemize}
\end{itemize}


\subsection{Linear Regression}

\begin{theorem}\emph{(Ordinary Least Squares)}

	\noindent(i) In one-factor setting, maximum likelihood estimators for slope, intercept and variance are given by (see (\ref{sample_variance}, \ref{sample_correlation})):
	\begin{equation*}
		\hat{\beta} = \frac {S_Y r_{X,Y}}{S_X},\ \ \hat{\alpha}= \bar Y - \hat{\beta}\bar{X},\ \ \hat{\sigma}^2 = \frac 1n\sum_{i = 1}^n = (Y_i - \hat\beta X_i - \hat\alpha)^2
	\end{equation*}

	\noindent(ii) If the design matrix $X$ in a multiple linear regression has full rank, then the maximum likelihood estimators are given by:
	\begin{equation*}
		\hat{\mathbf{\beta}} = (X^TX)^{-1}(X^T Y), \ \ \hat{\sigma}^2 = \frac {\|Y-X\hat\beta\|^2}n
	\end{equation*}
\end{theorem}

\begin{theorem}\emph{(Weighted Least Squares/Heteroscedacity)}

	\noindent(i) Assume that error terms $\varepsilon_i$ are have variance as $\sigma_i^2 \equiv z_i\sigma^2$ for known constants $z_i$. Let $w_i \eq (z_i\sigma^2)^{-1}$, then maximum likelihood estimators for slope, intercept and variance are given by (see (\ref{sample_variance}, \ref{sample_correlation})):
	\begin{align*}
		\tilde{\beta} = \frac{\sum w_i(x - \tilde x)(y - \tilde y)}{\sum w_i (x - \tilde x)^2} =& \frac{\sum w_i\sum w_i x_i y_i - \sum w_ix_i\sum w_i y_i}{\sum w_i\sum w_i x_i^2 - \left(\sum w_ix_i\right)^2}\\
		\tilde\alpha =& \tilde y - \tilde \beta\tilde x\\
		\hat\sigma^2 =& n^{-1}\sum\frac 1 z_i(y_i - \tilde \beta x_i - \tilde \alpha)^2
	\end{align*}

	\noindent(ii) For the multi-factor model, maximum likelihood estimators can be written in the form:
	\begin{equation*}
		\tilde \beta = (X^TWX)^{-1}(X^T WY)
	\end{equation*}
\end{theorem}




\begin{theorem}
	Let $V\eq \operatorname{span}(X)$, and $V_0\subset V$. Denote the projection onto $V$ by $P_V$.
	\begin{enumerate}
		\item The likelihood ratio statistic for $H_0:X \beta_0 \in V_0$ equals
	\begin{equation*}
		2\log\lambda_n(X, Y) = n\log\frac{\|(E - P_{V_0})Y\|^2}{\|(E - P_{V})Y\|^2},
	\end{equation*}
	\item Under the null hypothesis, the following quantity has $F_{n-p, p-p_0}$ distribution:
	\begin{equation*}
		\frac{\|(P_V - P_{V_0})Y\|^2/(p-p_0)}{\|(E - P_{V})Y\|^2/(n-p)}
	\end{equation*}
\end{enumerate}
\end{theorem}


\section{Statistical tests}
\subsection{$t$-tests}
\subsubsection{One-sample $t$-test}
Let $X_1, X_2,\ldots X_n$ be an i.i.d. sample from the $N(\mu, \sigma^2)$-distribution with $\mu$ and $\sigma^2$ unknown. Given $\mu_0\in\RR$ we test:
\begin{equation}\label{ttest}
	H_0:\mu\leq \mu_0 \text{ against } H_1: \mu>\mu_0
\end{equation}
Test statistic:
\begin{equation}\label{tstatistic}
	T = \sqrt{n}\frac{\bar X_n - \mu_0}{S_X}
\end{equation}
By Theorem \ref{normaliid}, under $\mu = \mu_0$ the statistic has Student's $t_{n-1}$ distribution, consequently we can use.
\begin{equation}
	\sup_{\mu\leq\mu_0}\PP \left(T \geq t_{n-1, 1-\alpha}\right) \leq \alpha
\end{equation}

\subsubsection{t-Test for paired observations}
Let $(X_1, Y_1), (X_2, Y_2),\ldots (X_n, Y_n)$ be the i.i.d. sample of paired observations. We assume that $Z_i \eq X_i - Y_i$ is $N(\Delta, \sigma^2)$ is normally distributed, and the ordinary One-sample $t$-test can be used to test the null hypotheses $H_0:\Delta\geq0$. Note that if $X_i$ and $Y_i$ are strongly correlated then variance of $Z_i$ decreases and this improves the power of the $t$-test.

\subsubsection{Two-sample $t$-test}
Let $X_1, X_2,\ldots X_n$ and $Y_1, Y_2,\ldots Y_m$ be two mutually independent i.i.d samples from $N(\mu, \sigma^2)$ and $N(\nu, \sigma^2)$. The test checks
\begin{equation}
	H_0:\mu-\nu \leq 0 \text{ against } H_1: \mu-\nu>0
\end{equation}
Test statistic:
\begin{align}
	T =& \frac{\bar{X} - \bar{Y}}{S_{X,Y}\sqrt{\frac1n+\frac1m}}\\
	S_{X,Y}^2 = \frac1{m+n-2}&\left(\sum_{i=1}^n (X_i - \bar X_n)^2+ \sum_{j=1}^m (Y_j - \bar Y_m)^2\right)
\end{align}
Theorem \ref{normaliid} implies that $S_{X,Y}^2$ follows $\sigma^2 \cdot \chi^2_{m+n-2}$ distribution.

\subsection{Kolmogorov-Smirnov test}
Given and i.i.d. sample $X_1, \ldots X_n$ from some unknown distribution $F$, we want to test:
\begin{equation}\label{kstest}
H_0: F = F_0 \text{ against } H_1: F\neq F_0
\end{equation}
The test statistic is given by
\begin{equation}\label{ksstatistic}
	T = \sup_{x\in\RR} |\hat F_n(x) - F(x)|,
\end{equation}
where $\hat F_n(x)$ stands for the empirical distribution function (see \ref{ecdf}). The distribution of $T$ is the same for every continuous cdf $F_0$. The following limit establishes the test:
\begin{equation}
\lim_{n\rightarrow \infty}\PP_{F_0}\left(T > \frac zn\right) = 2\sum_{j = 1}^\infty(-1)^{j+1}e^{-j^2z^2}
\end{equation}





\end{document}
