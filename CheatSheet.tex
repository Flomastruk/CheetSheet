\documentclass[12pt]{article}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{mathtools}

\usepackage{wrapfig, tikz, tikz-cd}
\usetikzlibrary{arrows, arrows, calc, decorations.markings, automata,calc}

\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{colorlinks=false,linkbordercolor=red,linkcolor=green,pdfborderstyle={/S/U/W 2}}

\usepackage{url}




\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{conjecture}[theorem]{Conjecture}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\numberwithin{equation}{section}

\newcommand{\dd}{\partial}
\newcommand{\argmax}{\operatorname{argmax}}
\newcommand{\Span}{\operatorname{Span}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\KK}{\mathbb{K}}
\newcommand{\rr}{\text{r}}
\newcommand{\al}{\alpha}
\newcommand{\be}{\beta}
\newcommand{\eps}{\varepsilon}
\newcommand{\ph}{\varphi}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\BB}{\mathfrak{B}}
\newcommand{\MM}{\mathfrak{M}}
\newcommand{\kk}{\textbf{k}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\II}{\mathbb{I}}
\newcommand{\eq}{\stackrel{\mathclap{\normalfont\text{def}}}{=}}


\begin{document}

\section{General definitions}

\subsection{Basic}

\begin{itemize}
\item
Sample variance
\begin{equation}\label{sample_variance}
S_X^2 = \frac 1{n-1}\sum_{i = 1}^n(X_i - \bar X)^2
\end{equation}

\item
Sample correlation coefficient
\begin{equation}
r_{X,Y} = \frac{\sum_{i = 1}^{n}(X_i-\bar{X})(Y_i-\bar{Y})}{(n-1)\sqrt{(S_X^2S_Y^2)}}
\end{equation}

\item
QQ-plot for cumulative distribution function $F$ is the set of points $\left(q_F\left(\frac i {n+1}\right), x_{(i)}\right)$, where $q_F(\cdot)$ is the quantile function for the distribution.

\item Mean Squared Error (MSE)
\begin{equation}
	\text{MSE}(\theta; T(X), g(\theta)) = \mathbb{E}_\theta\left(T(X) - g(\theta)\right)^2
\end{equation}

\item
Bias-variance decomposition
\begin{equation}
\text{MSE}(\theta; T(X)) = \text{var}_\theta T + \left(\mathbb{E}_\theta T(X) - g(\theta)\right)^2
\end{equation}

\item\label{ecdf}
Empirical distribution function
\begin{equation}
	\hat{F_n}(x) = \sum_{i = 1}^n \II(X_i \leq x)
\end{equation}

\end{itemize}

\subsection {$k$-th order statistic $X_{(k)}$}
$X_{(k)}$ --- $k-th$ order statistic distribution for $n$ i.i.d. variables from continuous distribution $F$.
\begin{equation}\label{kth_order_pdf}
	f_{(k)}(x) = \frac{n!}{(k-1)!(n-k)!}F(x)^{k-1}(1-F(x))^{n-k}f(x)
\end{equation}
\begin{equation}\label{kth_order_cdf}
	F_{(k)}(x) = \sum_{j = k}^n {{n}\choose{j}}F(x)^j (1-F(x))^{n-j}
\end{equation}

\begin{equation}\label{kth_order_expectation}
	\mathbb{E}F(X_{(k)}) = \frac k{n+1}
\end{equation}




\section{Important distributions}
\begin{itemize}
	\item Student's $t$-distribution $t_\nu$, $\nu\in\RR_{>0}$
	\begin{itemize}
		\item pdf
		\begin{equation}\label{t_pdf}
			\frac{\Gamma(\frac{\nu+1}2)}{\sqrt{\pi\nu}\ \Gamma(\frac\nu2)}\left(1+\frac {x^2}\nu\right)^{-\frac{\nu+1}2}
		\end{equation}
		\item cdf
		\begin{equation}\label{t_cdf}
			\frac 12 +x\Gamma\left(\frac{\nu+1}2\right)\frac{_2F_1\left(\frac 12, \frac {\nu+1}2; \frac 32, -\frac {x^2}\nu\right)}{\sqrt{\pi\nu}\ \Gamma(\frac\nu2)}
		\end{equation}
		\item $t$-distribution with $n\in\NN$ degrees of freedom arises from the ratio of independent $N(0,1)$- and $\chi^2_n$-distributions
	\end{itemize}
	\item Poisson distribution Poisson$(\lambda),\ \lambda > 0$
	\begin{itemize}
		\item $\lambda$ is the average number of events per interval
		\item pdf
		\begin{equation}\label{poisson_pdf}
			p_\lambda(k) = e^{-\lambda}\frac {\lambda^k}{k!}
		\end{equation}
	\end{itemize}


	\item Geometric distribution $G(\theta),\ 0\leq\theta\leq 1$
	\begin{itemize}
		\item pdf
		\begin{equation}\label{geometric_pdf}
			f_\theta(k) = (1-\theta)^{1-k}\theta
		\end{equation}
		\item cdf
		\begin{equation}\label{geometric_cdf}
			F_\theta(k) = 1-(1-\theta)^k
		\end{equation}
	\end{itemize}

	\item Exponential distribution $F(x;\lambda)$
	\begin{itemize}
		\item pdf
		\begin{equation}\label{exponential_pdf}
			f_\lambda(x) = \lambda e^{-\lambda x}
		\end{equation}
		\item cdf
		\begin{equation}\label{exponential_cdf}
			F_\lambda(x) = 1 - e^{-\lambda x}
		\end{equation}
		\item $\EE_\lambda X = 1/\lambda$

	\end{itemize}

	\item Beta distribution $B(\alpha, \beta),\ \alpha, \beta > 0$
	\begin{itemize}
		\item pdf
		\begin{equation}\label{beta_pdf}
			f_{\alpha, \beta}(x) = \frac{x^{\alpha-1} (1-x)^{\beta-1}}{B(\alpha, \beta)},\ B(\alpha, \beta) \equiv \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}
		\end{equation}

	\end{itemize}

	\item Weibull distribution
	\begin{itemize}
		\item $\alpha$ and $\lambda$ are the ``shape'' and `inverse scale'' parameters.
		\item pdf
		\begin{equation}\label{weibull_pdf}
			f_{\lambda, \alpha}(x) = \lambda^{\alpha}\alpha x^{\alpha-1} e^{-(\lambda x)^{\alpha}}
		\end{equation}
		\item cdf
		\begin{equation}\label{weibull_cdf}
			F_{\lambda, \alpha}(x) = 1 - e^{-(\lambda x)^\alpha}
		\end{equation}
	\end{itemize}

	\item Gamma distribution $\Gamma(\alpha, \lambda),\ \alpha >0, \lambda > 0$
	\begin{itemize}
		\item $\alpha$ and $\lambda$ are known as ``shape'' and ``inverse scale'' parameters.
		\item pdf
			\begin{equation}\label{gamma_pdf}
				f_{\alpha, \lambda} (x)= \frac{x^{\alpha - 1} \lambda^{\alpha}e^{-\lambda x}}{\Gamma(\alpha)}
			\end{equation}
		\item cdf (where $\gamma(s, x) = \int_0^x t^{s-1}e^{-t}dt$ --- is the ``incomplete gamma function'')
			\begin{equation}
				F_{\alpha,\lambda}(x) = \frac{\gamma(\alpha, x\beta)}{\Gamma(\alpha)}
			\end{equation}


	\end{itemize}
\end{itemize}

\section{Fundamental results}
\begin{theorem}\label{normaliid}
	Let $X_1,\ldots X_n$ be an i.i.d. ramdom variables from the $N(\mu, \sigma^2)$ distribution, then
	\begin{enumerate}
		\item $\bar X$ is $N(\mu, \sigma^2/n)$ distributed;
		\item $(n-1)S_X^2/\sigma^2$ is $\chi^2_{n-1}$-distributed (see \ref{sample_variance});
		\item $\bar X$ and $S^2_{X}$ are independent;
		\item $\sqrt{n}(\bar{X} - \mu)/\sqrt{S_X^2}$ has the $t_{n-1}$- distribution.
	\end{enumerate}
\end{theorem}
\proof $\left\|X\right\|^2 - n \bar{X}^2 = (n-1)S_X^2$


\section{Estimators}

\subsection{Maximum of $n$ uniformally distributed statistics}

Set up: $X_1, X_2,\ldots X_n$ i.i.d. drown from $U[0, \theta]$, where $\theta$ is the parameter of interest.

\begin{itemize}
	\item $\hat\theta = 2\bar{X_n}$
	\begin{itemize}
		\item method of moments estimator
		\item \emph{unbiased}
		\item $\text{MSE}(\theta, \hat\theta) = \frac {\theta^2}{3n}$, see (\ref{kth_order_pdf})
	\end{itemize}
	\item $X_{(n)}$ --- $n$-th order statistic, i.e. maximum.
	\begin{itemize}
		\item $\mathbb{E}_\theta X_{(n)} = \frac{n}{n+1}\theta$, see (\ref{kth_order_pdf})
		\item $\text{MSE}(\theta, X_{(n)}) = \frac {2\theta^2}{(n+2)(n+1)}$
	\end{itemize}
	\item $\frac{n+2}{n+1}X_{(n)}$
	\begin{itemize}
		\item best estimator of the form $cX_{(n)}$
		\item $\text{MSE}(\theta, \frac{n+2}{n+1}X_{(n)}) = \frac {\theta^2}{(n+1)^2}$
	\end{itemize}
\end{itemize}

\subsection{Univariate normal distribution}

\begin{itemize}
	\item $\displaystyle{(\hat\mu, \hat{\sigma}^2) = \left(\bar{X_n}, \frac 1 n \sum_{i=1}^n (X_i - \bar{X_n}) \right) = \left(\bar{X_n}, \frac {n-1} n S_X^2 \right)}$
	\begin{itemize}
		\item maximum likelihood estimator
		\item method of moments estimator
		\item $\hat\mu$ is \emph{unbiased}
		\item $\mathbb{E}_{(\mu,\sigma^2)}\hat{\sigma}^2 = \frac {n-1} n \sigma^2$

	\end{itemize}

\end{itemize}

\subsection{Empirical distribution function}
Let $X_1, \ldots X_n$ be an i.i.d. sample drawn from the distribution $F$.
\begin{itemize}
	\item The empirical distribution function (ecdf) $\hat{F}(x) = \sum_{i = 1}^n \II(X_i \leq x)$ (see \ref{ecdf})
	\begin{itemize}
		\item \emph{unbiased}
		\item $\textbf{cov}_F\left(\hat F (u), \hat F (v)\right) = n^{-1}(F(\min(u, v)) - F(u)F(v))$ -- positively correlated
	\end{itemize}
\end{itemize}



\section{Statistical tests}
\subsection{$t$-tests}
\subsubsection{One-sample $t$-test}
Let $X_1, X_2,\ldots X_n$ be an i.i.d. sample from the $N(\mu, \sigma^2)$-distribution with $\mu$ and $\sigma^2$ unknown. Given $\mu_0\in\RR$ we test:
\begin{equation}\label{ttest}
	H_0:\mu\leq \mu_0 \text{ against } H_1: \mu>\mu_0
\end{equation}
Test statistic:
\begin{equation}\label{tstatistic}
	T = \sqrt{n}\frac{\bar X_n - \mu_0}{S_X}
\end{equation}
By Theorem \ref{normaliid}, under $\mu = \mu_0$ the statistic has Student's $t_{n-1}$ distribution, consequently we can use.
\begin{equation}
	\sup_{\mu\leq\mu_0}\PP \left(T \geq t_{n-1, 1-\alpha}\right) \leq \alpha
\end{equation}

\subsubsection{t-Test for paired observations}
Let $(X_1, Y_1), (X_2, Y_2),\ldots (X_n, Y_n)$ be the i.i.d. sample of paired observations. We assume that $Z_i \eq X_i - Y_i$ is $N(\Delta, \sigma^2)$ is normally distributed, and the ordinary One-sample $t$-test can be used to test the null hypotheses $H_0:\Delta\geq0$. Note that if $X_i$ and $Y_i$ are strongly correlated then variance of $Z_i$ decreases and this improves the power of the $t$-test.

\subsubsection{Two-sample $t$-test}
Let $X_1, X_2,\ldots X_n$ and $Y_1, Y_2,\ldots Y_m$ be two mutually independent i.i.d samples from $N(\mu, \sigma^2)$ and $N(\nu, \sigma^2)$. The test checks
\begin{equation}
	H_0:\mu-\nu \leq 0 \text{ against } H_1: \mu-\nu>0
\end{equation}
Test statistic:
\begin{align}
	T =& \frac{\bar{X} - \bar{Y}}{S_{X,Y}\sqrt{\frac1n+\frac1m}}\\
	S_{X,Y}^2 = \frac1{m+n-2}&\left(\sum_{i=1}^n (X_i - \bar X_n)^2+ \sum_{j=1}^m (Y_j - \bar Y_m)^2\right)
\end{align}
Theorem \ref{normaliid} implies that the squared denomenator follows $\sigma^2 \cdot \chi^2_{m+n-2}$ distribution.

\subsection{Kolmogorov-Smirnov test}
Given and i.i.d. sample $X_1, \ldots X_n$ from some unknown distribution $F$, we want to test:
\begin{equation}\label{kstest}
H_0: F = F_0 \text{ against } H_1: F\neq F_0
\end{equation}
The test statistic is given by
\begin{equation}\label{ksstatistic}
	T = \sup_{x\in\RR} |\hat F_n(x) - F(x)|,
\end{equation}
where $\hat F_n(x)$ stands for the empirical distribution function (see \ref{ecdf}). The distribution of $T$ is the same for every continuous cdf $F_0$. The following limit establishes the test:
\begin{equation}
\lim_{n\rightarrow \infty}\PP_{F_0}\left(T > \frac zn\right) = 2\sum_{j = 1}^\infty(-1)^{j+1}e^{-j^2z^2}
\end{equation}





\end{document}
